{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from my_toolkit import key_check\n",
    "from urllib.parse import quote_plus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys loaded correctly\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables from the .env in the local environment    \n",
    "my_env_path= 'C:\\src\\\\ai\\data-sourcing-challenge\\.data-sourcing_challenge.env'\n",
    "'''\n",
    "def check_keys(key_path=None):\n",
    "    try:\n",
    "        load_dotenv(key_path,override=True)   \n",
    "        nyt_api_key = os.getenv(\"NYT_API_KEY\")\n",
    "        tmdb_api_key = os.getenv(\"TMDB_API_KEY\")\n",
    "        assert nyt_api_key is not None, 'NYT_API_KEY not found in .env file'\n",
    "        assert tmdb_api_key is not None, 'TMDB_API_KEY not found in .env file'\n",
    "        responce=requests.get(f'https://api.nytimes.com/svc/mostpopular/v2/viewed/1.json?api-key={nyt_api_key}')\n",
    "        assert responce.status_code == 200, f'The key provided failed to authenticate nyt_api_key {nyt_api_key} code {responce.status_code}'\n",
    "        responce=requests.get(f'https://api.themoviedb.org/3/movie/11?api_key={tmdb_api_key}')\n",
    "        assert responce.status_code == 200, f'The key provided failed to authenticate tmdb_api_key {tmdb_api_key} code {responce.status_code}'\n",
    "    except Exception as e:\n",
    "        # Handle potential errors in loading .env or missing API keys\n",
    "        print(f'An error occurred: {e}')\n",
    "    else:\n",
    "        print('All keys laoded correctly')\n",
    "'''\n",
    "if key_check(my_env_path):\n",
    "    nyt_api_key = os.getenv(\"NYT_API_KEY\")\n",
    "    tmdb_api_key = os.getenv(\"TMDB_API_KEY\")\n",
    "else:\n",
    "    print ('fix Keys and rerun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the New York Times API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=section_name%3A%22Movies%22+AND+type_of_material%3A%22Review%22+AND+headline%3A%22love%22&begin_date=20130101&end_date=20230531&fl=headline%2Cweb_url%2Csnippet%2Csource%2Ckeywords%2Cpub_date%2Cbyline%2Cword_count&sort=newest'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample from web page\n",
    "# https://api.nytimes.com/svc/search/v2/articlesearch.json?q=new+york+times&page=2&sort=oldest&api-key=your-api-key \n",
    "\n",
    "#  Set the base URL\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "\n",
    "# Filter for movie reviews with \"love\" in the headline\n",
    "# section_name should be \"Movies\"\n",
    "# type_of_material should be \"Review\"\n",
    "filter_query = quote_plus('section_name:\"Movies\" AND type_of_material:\"Review\" AND headline:\"love\"')\n",
    "\n",
    "# Use a sort filter, sort by newest\n",
    "sort = \"newest\"\n",
    "\n",
    "# Select the following fields to return:\n",
    "# headline, web_url, snippet, source, keywords, pub_date, byline, word_count\n",
    "field_list = quote_plus(\"headline,web_url,snippet,source,keywords,pub_date,byline,word_count\")\n",
    "\n",
    "# Search for reviews published between a begin and end date\n",
    "begin_date = \"20130101\"\n",
    "end_date = \"20230531\"\n",
    "\n",
    "# Build URL\n",
    "query_url=(f'{url}fq={filter_query}&begin_date={begin_date}&end_date={end_date}&fl={field_list}&sort={sort}')\n",
    "display (query_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked page  0\n",
      "Checked page  1\n",
      "Checked page  2\n",
      "Checked page  3\n",
      "Checked page  4\n",
      "Checked page  5\n",
      "Checked page  6\n",
      "Checked page  7\n",
      "Checked page  8\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the reviews\n",
    "love_movie_review=[]\n",
    "# loop through pages 0-19\n",
    "for page in range(0,20):\n",
    "    # Construct the query URL for the current page\n",
    "    page_url = f'{query_url}&page={page}&api-key={nyt_api_key}'\n",
    "\n",
    "    try:\n",
    "        # Attempt to make a \"GET\" request and parse the JSON response\n",
    "        response = requests.get(page_url).json()\n",
    "\n",
    "        # Check if the \"docs\" list is empty; if so, print a message and exit the loop\n",
    "        if not (response[\"response\"][\"docs\"]):\n",
    "                raise ValueError (f'No results on page {page}, stopping.')\n",
    "        else: \n",
    "            print(f'Checked page  {page}')            \n",
    "        # Otherwise, process each article in \"docs\"\n",
    "        for article in response[\"response\"][\"docs\"]:\n",
    "            if article :\n",
    "                love_movie_review.append(article)\n",
    "            # diag print(f'article title {article[\"headline\"][\"main\"]}')\n",
    "        # Add a twelve second pause between requests to adhere to API query limits\n",
    "        time.sleep(12)\n",
    "    except ValueError as e:\n",
    "        # Handle the case where no documents are found\n",
    "        print(e)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        # Handle potential errors in the request or data processing\n",
    "        print(f'An error occurred: {e}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 5 results in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n",
    "print (json.dumps(love_movie_review[:5], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert reviews_list to a Pandas DataFrame using json_normalize()\n",
    "love_movie_review_df=pd.json_normalize(love_movie_review)\n",
    "love_movie_review_df.iloc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title from the \"headline.main\" column and\n",
    "# \n",
    "# Regular expression to match text enclosed by ‘ and ’\n",
    "# save it to a new column \"title\"\n",
    "# Title is between unicode characters \\u2018 and \\u2019.\n",
    "# In testing I found that some reviews did not have titles.\n",
    "# -----End string should include \" Review\" to avoid cutting title early----\n",
    "# \"\" Review doesnt work \"Review\": ‘What’s Love Got to Do With It?’  not all titles come first. opted for u0020 before and after. \n",
    "pattern = r\"(?:\\u0020|^)\\u2018(.+?)\\u2019(?:\\u003A|\\u0020|$)\"\n",
    "\n",
    "def extract_title(headline):\n",
    "    title_match = re.search(pattern, headline)\n",
    "    if title_match:\n",
    "        return title_match.group(1)\n",
    "    else:\n",
    "        print (headline)\n",
    "        return \"not found\"\n",
    "\n",
    "# Apply this function to the 'headline.main' column and assign the result to the 'title' column\n",
    "love_movie_review_df['title'] = love_movie_review_df['headline.main'].apply(extract_title)\t\n",
    "love_movie_review_df[['snippet','title']].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the title from the \"headline.main\" column and\n",
    "# save it to a new column \"title\"\n",
    "# Title is between unicode characters \\u2018 and \\u2019.\n",
    "# -----End string should include \" Review\" to avoid cutting title early----\n",
    "# \n",
    "# In testing I found that some reviews did not have titles.\n",
    "# I found that some responses did not have title. The revised code prints a message with the headline  page and a message saying title not found. \n",
    "# \"\" Review doesnt work \"Review\": ‘What’s Love Got to Do With It?’  not all titles come first. \n",
    "# opted for u0020 before and after. \n",
    "\n",
    "pattern = r\"(?:\\u0020|^)\\u2018(.+?)\\u2019(?:\\u003A|\\u0020|$)\"\n",
    "\n",
    "def extract_title(headline, idx):\n",
    "    title_match = re.search(pattern, headline)\n",
    "    if title_match:\n",
    "        # print(idx,headline)\n",
    "        return title_match.group(1)\n",
    "    else:\n",
    "        print (f'not found {idx, headline}')\n",
    "    #    return \"not found\"\n",
    "\n",
    "# Apply this function to the 'headline.main' column and assign the result to the 'title' column\n",
    "# Direct iteration over the column to access both the index and value\n",
    "love_movie_review_df['title'] = love_movie_review_df.apply(lambda row: extract_title(row['headline.main'], row.name), axis=1)\n",
    "display (love_movie_review_df[['snippet','title']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'name' and 'value' from items in \"keywords\" column\n",
    "def extract_keywords(keyword_list):\n",
    "    extracted_keywords = \"\"\n",
    "    extracted_keywords = \";\".join(f\"{item['name']}: {item['value']}\" for item in keyword_list)\n",
    "    # extracted_keywords = extracted_keywords.rstrip(';')\n",
    "    return extracted_keywords\n",
    "# this copy make it possible for me to rerun this without having to start from scratch\n",
    "lmr_keyword_df = love_movie_review_df.copy(deep=True)\n",
    "\n",
    "# Fix the \"keywords\" column by converting cells from a list to a string\n",
    "lmr_keyword_df['keywords'] = lmr_keyword_df['keywords'].apply(extract_keywords)\n",
    "# display(lmr_keyword_df[['title','keywords']].head(3))\n",
    "#\n",
    "#play time with style\n",
    "styled_subset_df=lmr_keyword_df.loc[:4,['title','keywords']]\n",
    "styled_df = styled_subset_df.style.set_table_styles({\n",
    "    'title': [{'selector': '',\n",
    "                'props': [('width', '200px'), ('text-align', 'right')]}],\n",
    "    'keywords': [{'selector': '',\n",
    "                'props': [('width', '700px'), ('text-align', 'left')]}]\n",
    "}, overwrite=False).hide(axis=0)\n",
    "# display(styled_df)\n",
    "display (lmr_keyword_df)\n",
    "display (styled_df)\n",
    "del styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list from the \"title\" column using to_list()\n",
    "# These titles will be used in the query for The Movie Database\n",
    "titles_list = love_movie_review_df['title'].to_list()\n",
    "print(\"Top 5 Titles:\\n\" + json.dumps(titles_list[:5], indent= 4, ensure_ascii=False)[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access The Movie Database API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare The Movie Database query\n",
    "url_query   = \"https://api.themoviedb.org/3/search/movie?query=\"\n",
    "url_detail  = \"https://api.themoviedb.org/3/movie/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the results\n",
    "tmbd_movies_list=[]\n",
    "\n",
    "# Create a request counter to sleep the requests after a multiple\n",
    "# of 50 requests\n",
    "req_counter = 0\n",
    "'''\n",
    "https://developer.themoviedb.org/docs/rate-limiting\n",
    "While our legacy rate limits have been disabled for some time, \n",
    "we do still have some upper limits to help mitigate needlessly high bulk scraping. \n",
    "They sit somewhere in the 50 requests per second range. \n",
    "This limit could change at any time so be respectful of the service we have built and respect the 429 if you receive one.\n",
    "'''\n",
    "# Loop through the titles\n",
    "for title in titles_list:\n",
    "    # Check if we need to sleep before making a request\n",
    "    if req_counter % 50 == 0:\n",
    "        print ('naptime')\n",
    "        time.sleep(1)\n",
    "        req_counter=0\n",
    "    else:\n",
    "    # Add 1 to the request counter\n",
    "        req_counter += 1\n",
    "    # Perform a \"GET\" request for The Movie Database\n",
    "    response=requests.get(f'{url_query}{title}\"&api_key={tmdb_api_key}')\n",
    "\n",
    "    # Include a try clause to search for the full movie details.\n",
    "    # Use the except clause to print out a statement if a movie\n",
    "    # is not found.\n",
    "    try:\n",
    "        # Get movie id    \n",
    "        if response.status_code == 200:\n",
    "        # Get movie id\n",
    "            data = response.json()\n",
    "            if data['results']:\n",
    "                movie_id = data['results'][0]['id']\n",
    "                # print(f\"Movie ID Found: {movie_id}  {req_counter}  {title}\")\n",
    "        # Make a request for the full movie details\n",
    "        # Execute \"GET\" request with url\n",
    "                detail_response = requests.get(f\"{url_detail}{movie_id}?api_key={tmdb_api_key}\")\n",
    "                if detail_response.status_code == 200:\n",
    "                    movie_detail_df = detail_response.json()\n",
    "                    # display (detail_response.json())\n",
    "                    # print (json.dumps(movie_detail_df,indent=4))\n",
    "\n",
    "        # Extract the genre names into a list\n",
    "                    genres_list = [genre['name'] for genre in movie_detail_df['genres']]\n",
    "                    # print (genres_list)\n",
    "        # Extract the spoken_languages' English name into a list\n",
    "                    spoken_languages = [spoken_language['english_name'] for spoken_language in movie_detail_df['spoken_languages']]\n",
    "                    # print(spoken_languages)\n",
    "\n",
    "        # Sample 'spoken_languages': [{'english_name': 'Spanish', 'iso_639_1': 'es', 'name': 'Español'}],\n",
    "        # Extract the production_countries' name into a list\n",
    "                    production_countries_name = [production_countries_name['name'] for production_countries_name in movie_detail_df['production_countries']]\n",
    "                    \n",
    "                    # 'production_countries': [{'iso_3166_1': 'AR', 'name': 'Argentina'}],\n",
    "                    # print(production_countries_name)\n",
    "        # Add the relevant data to a dictionary and\n",
    "        \n",
    "        # append it to the tmdb_movies_list list\n",
    "                    tmbd_movies_list.append({\n",
    "                    'movie_id'        : movie_id,\n",
    "                    'movie_title'     : title,\n",
    "                    'original_title'  : (movie_detail_df['original_title']),\n",
    "                    'budget'          : (movie_detail_df['budget']),\n",
    "                    'gendre_list'     : genres_list,\n",
    "                    'original_language'        : (movie_detail_df['original_language']),\n",
    "                    'spoken_languages': spoken_languages,\n",
    "                    'homepage'        : (movie_detail_df['homepage']),\n",
    "                    'overview'        : (movie_detail_df['overview']),\n",
    "                    'popularity'      : (movie_detail_df['popularity']),\n",
    "                    'runtime'         : (movie_detail_df['runtime']),\n",
    "                    'revenue'         : (movie_detail_df['revenue']),\n",
    "                    'release_date'    : (movie_detail_df['release_date']),\n",
    "                    'vote_average'    : (movie_detail_df['vote_average']),\n",
    "                    'vote_count'      : (movie_detail_df['vote_count']),\n",
    "                    'production_counties_name': production_countries_name})\n",
    "                    \n",
    "                else:\n",
    "                    raise (\"put a exception here\") \n",
    "        # Print out the title that was found\n",
    "            print (f'Found  {title}')\n",
    "        else:\n",
    "            print(f\"Failed to get movie ID, status code: {response.status_code}      {title}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "    # This catches all exceptions that are requests-related\n",
    "    # Including connection errors, timeouts, etc.\n",
    "        print(\"A network error occurred. Please try again later.\")\n",
    "        print(e)\n",
    "display (tmbd_movies_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 5 results in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n",
    "print (json.dumps(tmbd_movies_list[:5],indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "tmdb_movies_df = pd.DataFrame(tmbd_movies_list)\n",
    "display (tmdb_movies_df.head(3))\n",
    "display (love_movie_review_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Merge and Clean the Data for Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the New York Times reviews and TMDB DataFrames on title\n",
    "tmdb_movies_df.rename(columns={'movie_title':'title'}, inplace=True)\n",
    "merged_df = pd.merge(tmdb_movies_df, love_movie_review_df, on='title')\n",
    "nyt_tmdb_df = merged_df.copy()\n",
    "print (merged_df.columns)\n",
    "print (merged_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove list brackets and quotation marks on the columns containing lists\n",
    "# Create a list of the columns that need fixing\n",
    "def contains_list(series):\n",
    "    return series.map(lambda x: isinstance(x, list)).any()\n",
    "\n",
    "columns_with_lists = [col for col in nyt_tmdb_df.columns if contains_list(nyt_tmdb_df[col])]\n",
    "\n",
    "print  (f'\\n{columns_with_lists}\\n' )\n",
    "# Create a list of characters to remove\n",
    "def clean_list_string(s):\n",
    "    if isinstance(s, list):\n",
    "        # Convert list to string and remove unwanted characters\n",
    "        return str(s).strip('[]').replace('\"', '').replace(\"'\", \"\")\n",
    "    return s\n",
    "\n",
    "# Apply the cleaning directly, no need for preliminary list check\n",
    "for col in columns_with_lists:\n",
    "   nyt_tmdb_df[col] = nyt_tmdb_df[col].apply(clean_list_string)\n",
    "\n",
    "# Display the fixed DataFrame\n",
    "print (nyt_tmdb_df.head(2))\n",
    "print (nyt_tmdb_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"byline.person\" column\n",
    "nyt_tmdb_df.drop(columns=['byline.person'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate rows and reset index\n",
    "print(nyt_tmdb_df[nyt_tmdb_df.duplicated(keep=False)])\n",
    "#duplicates = nyt_tmdb_df.duplicated()\n",
    "#print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV without the index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
